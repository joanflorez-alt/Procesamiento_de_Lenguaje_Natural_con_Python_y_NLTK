para buscar: cmd  +  F
para visualizar: cmd + altizq + v
para guardar: cmd + s

## Git Hub

Pasos para conectar tu proyecto a GitHub

1. Crea el repositorio en GitHub
Ve a github.com

Inicia sesi칩n.

Haz clic en "New" o "Nuevo repositorio"

Asigna un nombre, por ejemplo: Procesamiento_de_Lenguaje_Natural_con_Python_y_NLTK

No marques la opci칩n de crear README si ya tienes archivos locales.

Crea el repositorio y copia la URL SSH.

2. Desde la terminal: entra a la carpeta del proyecto local
```sh
cd ruta/a/tu/proyecto
```

3. Inicializa Git
```sh
git init
```

4. Agrega todos los archivos al 치rea de preparaci칩n
```sh
git add .
```

5. Haz el primer commit
```sh
git commit -m "primero 28_6_25"
```

6. Conecta tu proyecto local con el repositorio de GitHub
Aseg칰rate de copiar tu enlace SSH correcto. Por ejemplo:

```sh
git remote add origin git@github.com:joanflorez-alt/Procesamiento_de_Lenguaje_Natural_con_Python_y_NLTK.git
```

7. Env칤a tu proyecto a GitHub (rama main)
```sh
git push -u origin main
```

# Pr칩ximos pasos 칰tiles

游댲 Para subir futuros cambios:
```sh
git add .
git commit -m "Mi nuevo cambio"
git push
```

游댲 Para ver el estado actual del repositorio:
```sh
git status
```
游댲 Para clonar el repositorio en otro equipo:
```sh
git clone git@github.com:joanflorez-alt/Procesamiento_de_Lenguaje_Natural_con_Python_y_NLTK.git
```



## Procesamiento de Lenguaje Natural con Python y NLTK


쯈u칠 es el procesamiento del lenguaje natural?
El procesamiento del lenguaje natural (NLP) es un campo que fusiona ciencias de la computaci칩n, ling칲칤stica e inteligencia artificial. Su objetivo es lograr que las m치quinas interact칰en de manera efectiva con los humanos usando el lenguaje que utilizamos para comunicarnos d칤a a d칤a. Dentro del NLP, el Entendimiento del Lenguaje Natural (NLU, por sus siglas en ingl칠s) se dedica a tareas espec칤ficas que demuestran que una m치quina no solo puede procesar nuestro idioma, sino tambi칠n comprenderlo para responder de manera coherente.

쮺u치l es el papel del test de Turing en la inteligencia artificial?
Alan Turing, un matem치tico brillante de la Segunda Guerra Mundial, es conocido por haber contribuido al desarrollo de la inteligencia artificial. Su legado incluye el Test de Turing, que propone una idea sencilla y poderosa: si un humano no puede distinguir a una m치quina de una persona durante una conversaci칩n, la m치quina alcanza un nivel de inteligencia comparable al humano. Este test representa un hito en la identificaci칩n del lenguaje como una medida de la inteligencia de una m치quina.

Un ejemplo popular en la cultura es el test de Beuyk Kampff de la pel칤cula "Blade Runner", donde se verifica si un individuo es humano o un replicante. Esta referencia cultural refuerza la importancia del procesamiento del lenguaje natural en el avance hacia la identificaci칩n de la inteligencia artificial.

쮺u치les son las aplicaciones actuales del NLP?
El procesamiento del lenguaje natural se utiliza en diversas aplicaciones cotidianas:

Motores de b칰squeda: Google, por ejemplo, usa algoritmos de NLP para interpretar consultas y brindar respuestas precisas.

Traducci칩n autom치tica: Herramientas como Google Translate dependen del NLP para convertir texto entre diferentes idiomas.

Chatbots: Los asistentes virtuales conversacionales son uno de los usos m치s reconocidos del NLP.

An치lisis de discurso y reconocimiento del habla: Estos algoritmos identifican y transforman el lenguaje hablado en texto y viceversa.

쯇or qu칠 es dif칤cil el procesamiento del lenguaje natural?
El principal reto del NLP radica en la complejidad y ambig칲edad del lenguaje humano. Un ejemplo claro es interpretaciones posibles para un anuncio de trabajo que busque "gente para trabajar entre dieciocho y treinta a침os". Una persona entiende que se refiere a la edad del candidato, pero un algoritmo, sin contexto, podr칤a interpretar esto de manera distinta. El lenguaje humano es intr칤nsecamente difuso y requiere una comprensi칩n contextual que a menudo no est치 expl칤cita. Por ello, el avance en este campo es desafiante, pero tambi칠n crucial para mejorar la interacci칩n entre m치quinas y humanos.


# EVOLUCI칍N DEL NLP
![name](imagenes/image_1.png)

쮺칩mo ha evolucionado el procesamiento del lenguaje natural?
El procesamiento del lenguaje natural (PLN) ha recorrido un largo camino desde sus inicios. Desde las primeras propuestas de Turing en los a침os cincuenta, pasando por el auge de los sistemas basados en reglas, hasta las innovaciones actuales con el deep learning. Este fascinante viaje revela c칩mo la tecnolog칤a y la ling칲칤stica se han combinado para acercarnos a m치quinas que no solo entienden sino que tambi칠n interact칰an con nosotros.

쮺u치l fue el papel de los sistemas basados en reglas?
Al principio, los sistemas de PLN eran principalmente basados en reglas. Estas m치quinas no aprend칤an de forma aut칩noma; en cambio, se programaban con reglas ling칲칤sticas predefinidas. Esto les permit칤a generar conversaciones y reconocer si una frase estaba escrita o pronunciada correctamente, pero su capacidad era limitada. Fue un enfoque predominante hasta los a침os noventa, cuando la estad칤stica empez칩 a jugar un papel m치s importante.

쮺칩mo la estad칤stica transform칩 el PLN?
A partir de los a침os noventa, las t칠cnicas estad칤sticas comenzaron a dominar el PLN. Esto implic칩 entrenar algoritmos usando grandes corpus de texto, permitiendo a las m치quinas aprender distribuciones probables de palabras. Esta transici칩n marc칩 un cambio significativo: de basarse en reglas fijas a modelos que pod칤an comprender y generar frases con sentido gramatical mediante el aprendizaje de patrones.

쮼l rol de machine learning y deep learning en PLN?
A partir del a침o 2000, el machine learning tom칩 relevancia, ofreciendo algoritmos m치s sofisticados que se perfeccionaron hasta 2014. El verdadero punto de inflexi칩n fue la democratizaci칩n de las GPUs, permitiendo que el deep learning fuera accesible y revolucionara el PLN. Hoy en d칤a, el deep learning es considerado el estado del arte, posibilitando algoritmos altamente precisos y eficientes para la comprensi칩n y generaci칩n del lenguaje.

쯇or qu칠 importa el entendimiento de texto a bajo nivel?
El an치lisis preciso de texto a bajo nivel es crucial en PLN. Existen algoritmos dise침ados espec칤ficamente para tareas como clasificaci칩n de texto, an치lisis de sentimientos y comprensi칩n contextual. Aunque son extremadamente eficaces en sus tareas, suelen estar limitados a un solo tipo de tarea.

쯈u칠 son los modelos multitarea y cu치les son sus ventajas?
Los avances en aprendizaje de representaciones han dado lugar a modelos multitarea, que utilizan redes neuronales para realizar m칰ltiples funciones simult치neamente. Basados en conceptos matem치ticos de representaci칩n, estos modelos asignan vectores a palabras y frases para identificar similitudes sem치nticas, permitiendo un procesamiento m치s vers치til y eficaz.

쮺u치les son las innovaciones recientes en arquitecturas neuronales?
Desde 2018, las arquitecturas como las redes Transformer y Reformer han revolucionado el PLN gracias a los mecanismos de atenci칩n. Estos permiten un an치lisis m치s inteligente de las secuencias de texto, procesando solo las palabras relevantes dentro de una frase sin analizarla por completo. Aunque estos avances son notables, a칰n existe el desaf칤o de desarrollar algoritmos con comprensi칩n de sentido com칰n similar a la humana.

쮺칩mo puedes iniciarte en el procesamiento del lenguaje natural?
Hoy en d칤a, el aprendizaje de PLN comienza con el uso de dos librer칤as fundamentales: NLTK y spaCy. Estas herramientas ofrecen la base necesaria para explorar desde tareas simples hasta aplicaciones industriales m치s complejas.

쮺칩mo se estructura el camino de aprendizaje en este curso?
El curso se divide en bloques esenciales:

Fundamentos de PLN con NLTK: Para entender y usar la estad칤stica de corpus en tareas de PLN.

Aplicaciones: Usando NLTK para crear modelos que desempe침en diversas tareas de PLN.

NLP Industrial: C칩mo escalar modelos a aplicaciones reales usando spaCy.

Bloque avanzado: Combinando NLTK y spaCy para abordar conceptos m치s complejos en PLN.

Con este camino estructurado, estar치s preparado para sumergirte en el mundo del PLN y desarrollar aplicaciones pr치cticas que aprovechen la capacidad creciente de las m치quinas para entender el lenguaje humano. No dudes en dejar tus comentarios sobre qu칠 te entusiasma m치s de este curso y los proyectos que planeas crear.

![name](imagenes/image_2.png)


# Librerias
![name](imagenes/image_3.png)

## CONCEPTOS B츼SICOS NLP

쮺u치les son los fundamentos del procesamiento del lenguaje natural?
El procesamiento del lenguaje natural (PLN) es un campo fascinante que combina la ling칲칤stica y la inteligencia artificial para permitir que las m치quinas entiendan, interpreten y respondan al lenguaje humano de manera efectiva. En esta introducci칩n, abordaremos los conceptos b치sicos y la diferencia entre el PLN y la ling칲칤stica computacional, as칤 como algunos procesos clave involucrados en la normalizaci칩n de texto.

쮺u치l es la diferencia entre procesamiento del lenguaje natural y ling칲칤stica computacional?
Aunque la meta de ambas 치reas es comprender el lenguaje, sus enfoques difieren:

Procesamiento del lenguaje natural (PLN): Se centra en aplicaciones pr치cticas y est치 orientado hacia la ingenier칤a. Busca desarrollar sistemas que puedan comprender y generar lenguaje humano.
Ling칲칤stica computacional: Su enfoque es m치s cient칤fico, intentando resolver interrogantes del lenguaje mediante el estudio de c칩mo los humanos procesan y computan el lenguaje de manera innata.
쮺칩mo se crean modelos en ling칲칤stica computacional?
La ling칲칤stica computacional crea modelos a partir de dos enfoques principales:

Basado en conocimiento: Se centra en reglas y principios ling칲칤sticos.
Basado en datos: Utiliza aprendizaje autom치tico (machine learning) alimentado por grandes vol칰menes de datos, recurriendo al an치lisis estad칤stico.
쯈u칠 es la normalizaci칩n de texto?
La normalizaci칩n de texto es un proceso est치ndar en el procesamiento de lenguaje que incluye varias etapas de limpieza y transformaci칩n de texto para preparar datos textuales para el an치lisis.

쮼n qu칠 consiste la tokenizaci칩n?
La tokenizaci칩n divide un texto en unidades m칤nimas, com칰nmente conocidas como palabras o tokens. Por ejemplo, la frase "Mi hermano dej칩 de comer" se tokeniza en las siguientes palabras: "Mi", "hermano", "dej칩", "de", "comer".

쯈u칠 es la lematizaci칩n?
La lematizaci칩n transforma las palabras a su forma ra칤z o lema. Esto es especialmente 칰til para los verbos. En el ejemplo anterior, "dej칩" se lematiza a "dejar".

쮺칩mo se segmenta el texto en frases?
La segmentaci칩n en frases intenta dividir el texto en oraciones utilizando la puntuaci칩n. Sin embargo, este proceso puede ser complicado, ya que las reglas gramaticales no siempre son consistentes.

쯈u칠 es un corpus y c칩mo se diferencia de un corpora?
En la investigaci칩n del lenguaje, es importante conocer:

Corpus: Una colecci칩n de textos utilizada para analizar datos ling칲칤sticos. Proporciona el material necesario para construir modelos estad칤sticos.
Corpora: Plural de corpus, es decir, una colecci칩n de colecciones de texto.
Estos conceptos son fundamentales para entender c칩mo se estructuran y utilizan los datos en el PLN.

En esta etapa inicial del procesamiento del lenguaje, es esencial dominar estos fundamentos, ya que sientan la base para aplicar t칠cnicas m치s avanzadas de PLN. 춰Te animamos a seguir explorando y sumergi칠ndote en el apasionante mundo del procesamiento del lenguaje natural!

![name](imagenes/image_4.png)


![name](imagenes/image_5.png)

## Procesamiento de Lenguaje Natural con Python en Google Colab

La libreria NLTK ya trate texto incorporado corpus corpora

https://colab.research.google.com/drive/1NbB9Gw73N6t1gS_CDp1vhcJMNsDm1yZ8?usp=sharing


usaremos una base de datos en especial: 

nltk.download("cess_esp")


Ver:

https://docs.python.org/es/3.13/library/re.html


# Hasta el momento ya tomamos un corpus para analizar

## Expresiones Regulares en Python: Patrones de B칰squeda Avanzados

쮺칩mo usar expresiones regulares en NLTK para analizar un corpus?
En el fascinante mundo de la ling칲칤stica computacional, las expresiones regulares juegan un papel crucial para realizar b칰squedas avanzadas en grandes cantidades de texto, como un corpus. Si est치s interesado en descubrir c칩mo construir patrones de b칰squeda efectivos, est치s en el lugar adecuado. En esta gu칤a, desentra침amos el uso de las expresiones regulares con la librer칤a NLTK (Natural Language Toolkit) y Python, desde lo b치sico hasta los patrones m치s sofisticados.

쯈u칠 es una expresi칩n regular?
Las expresiones regulares son secuencias de caracteres que forman un patr칩n de b칰squeda. Utilizadas en el procesamiento de texto, permiten navegar y filtrar informaci칩n dentro de cadenas de texto para encontrar coincidencias espec칤ficas. Python cuenta con una poderosa librer칤a llamada re que facilita su incorporaci칩n.

쮺칩mo construir un arreglo filtrando palabras con expresiones regulares?
Comenzamos con la creaci칩n de un arreglo utilizando una expresi칩n regular simple. A continuaci칩n, se muestra c칩mo definir patrones de b칰squeda b치sicos y avanzados.

import re

# Ejemplo b치sico: Buscar palabras que contengan 'es'
AR = [w for w in flatten if re.search(r'es', w)]
En este fragmento de c칩digo, utilizamos la funci칩n re.search() de la librer칤a de expresiones regulares de Python, especificando un patr칩n y evaluando cada palabra en la lista flatten. Si el patr칩n 'es' se encuentra en una palabra, esta se a침ade al nuevo arreglo AR.

쮺칩mo redefinir patrones de b칰squeda utilizando metacaracteres?
Los metacaracteres enriquecen las b칰squedas con expresiones regulares al permitir una mayor especificidad, como buscar coincidencias al principio o al final de las palabras.

$: Marca el final de una cadena.

# Ejemplo: Solo palabras que terminan en 'es'
AR = [w for w in flatten if re.search(r'es$', w)]
^: Marca el inicio de una cadena.

# Ejemplo: Solo palabras que empiezan con 'es'
AR = [w for w in flatten if re.search(r'^es', w)]
쮺칩mo utilizar rangos en las expresiones regulares?
Los rangos permiten definir un conjunto de caracteres que pueden ocupar una posici칩n espec칤fica dentro de una cadena de texto:

# Ejemplo: Buscar palabras que comienzan con 'g', 'h' o 'i'
AR = [w for w in flatten if re.search(r'^[ghi]', w)]
En este ejemplo, creamos un arreglo donde filtramos palabras que empiezan con una letra ubicada entre 'g' y 'i'. Esto se logra definiendo un rango [ghi] como el primer car치cter en las palabras seleccionadas en el arreglo.

쯈u칠 son las clausuras en las expresiones regulares?
Las clausuras especifican el n칰mero de repeticiones permitidas para un patr칩n dado:

*: Permite que un patr칩n se repita cero o m치s veces.

# Ejemplo: Patrones que pueden aparecer cero o m치s veces
AR = [w for w in flatten if re.search(r'^no*', w)]
+: Requiere que un patr칩n se repita al menos una vez.

# Ejemplo: Patrones que deben aparecer al menos una vez
AR = [w for w in flatten if re.search(r'^no+', w)]
C칩mo seguir aprendiendo sobre expresiones regulares
춰Esto es solo el comienzo! Las expresiones regulares son una herramienta impresionante para aquellos apasionados por la computaci칩n y el an치lisis ling칲칤stico. A medida que avances, podr치s explorar c칩mo definir tokenizadores usando estas t칠cnicas, ayudando a estructurar texto de manera m치s eficiente. No dudes en experimentar y seguir practicando para dominar su uso. Tu viaje en el 치mbito del procesamiento de lenguaje natural ser치 incre칤blemente enriquecedor. 춰Adelante!


# otro ejemplo

arr = [w for w in flatten if re.search("^(no)*", w)]
print(arr[:20])


## Tokenizaci칩n de Texto con Expresiones Regulares en Python

쮺칩mo usar expresiones regulares para definir algoritmos de tokenizaci칩n en Python?
Las expresiones regulares son una herramienta poderosa que nos permite manipular texto de manera eficiente. En particular, cuando se trata de procesar lenguaje natural, podemos utilizarlas para definir algoritmos de tokenizaci칩n en Python. En esta clase, exploraremos c칩mo aprovechar las expresiones regulares con la librer칤a NLTK para tokenizar texto y limpiar nuestro corpus para an치lisis.

쯈u칠 es una cadena de texto RAW y c칩mo se utiliza?
Para que Python interprete una cadena de texto sin reconocer caracteres especiales, debemos utilizar el prefijo r delante de la cadena. Esto indica que Python debe tratar los caracteres especiales como texto plano, tambi칠n conocido como texto "RAW".

Por ejemplo:

print(r"Esta es una cadena con una nueva l칤nea \n que ser치 mostrada tal cual.")
El uso de la r convierte a los caracteres especiales en parte del texto y no en comandos ejecutables dentro de la cadena, lo que es esencial cuando tratamos con texto que incluye caracteres de escape.

쮺칩mo tokenizar texto con espacios vac칤os?
Tokenizar texto es el proceso de dividirlo en unidades m치s peque침as, conocidas como "tokens". El m칠todo m치s b치sico de tokenizaci칩n es mediante la separaci칩n por espacios vac칤os, y esto se puede lograr f치cilmente utilizando la funci칩n split de la librer칤a re.

import re

texto = "Cuando sea el rey del mundo, (imaginaba 칠l en su cabeza) no tendr칠 que preocuparme por estas bobadas."
tokens = re.split(r'\s+', texto)
Aqu칤, el uso de \s+ permite dividir el texto en base a uno o m치s espacios vac칤os, creando tokens individuales para cada palabra.

쮺칩mo mejorar la tokenizaci칩n con expresiones regulares?
Para lograr un nivel de tokenizaci칩n m치s sofisticado, podemos desarrollar expresiones regulares que filtren caracteres especiales no deseados.

Consideremos la siguiente expresi칩n regular:

tokens = re.split(r'[ \t\n]+', texto)
Esta expresi칩n busca espacios, tabulaciones y nuevas l칤neas, proporcionando una tokenizaci칩n m치s limpia al eliminar estos caracteres del proceso.

쮺칩mo crear un tokenizador sofisticado con NLTK?
A medida que los textos se vuelven m치s complejos, con caracteres y s칤mbolos especiales, las expresiones regulares b치sicas no son suficientes. Aqu칤 es donde NLTK ofrece una excelente soluci칩n con la funci칩n RegexpTokenizer.

Una aplicaci칩n avanzada de esta t칠cnica se puede ver como sigue:

from nltk.tokenize import RegexpTokenizer

pattern = r'\b\w+\b'
tokenizer = RegexpTokenizer(pattern)
tokens = tokenizer.tokenize(texto)
El patr칩n \b\w+\b permite capturar palabras completas, evitando caracteres que no contribuyen al contenido sem치ntico del texto.

쯇or qu칠 usar Regex Token Eyes de NLTK?
Cuando enfrentamos textos altamente complejos, con abreviaciones, n칰meros decimales y otros elementos, Regex Token Eyes de NLTK se convierte en un recurso inestimable. Este tokenizador emplea expresiones regulares complejas para lograr una tokenizaci칩n precisa.

from nltk.tokenize import RegexpTokenizer

pattern = r"\b\w+\b(?:[.\-]\w+)*"
tokenizer = RegexpTokenizer(pattern)
tokens = tokenizer.tokenize("En los EE.UU., esa postal vale quince cincuenta d칩lares...")
As칤, podemos capturar correctamente acr칩nimos, precios y otros casos especiales como tokens 칰nicos, mejorando significativamente la calidad del an치lisis.

Con herramientas como estas, est치s en capacidad de construir un tokenizador tanto b치sico como avanzado para diversas aplicaciones en procesamiento de lenguaje natural. 춰Sigue explorando y sorprendiendo al mundo con tus habilidades en Python!


## 쮺칩mo utilizar la estad칤stica en el procesamiento del lenguaje natural?
El procesamiento del lenguaje natural (NLP, por sus siglas en ingl칠s) es un campo fascinante que combina la inform치tica, la inteligencia artificial y la ling칲칤stica computacional para entender e interpretar el lenguaje humano. Un elemento clave en este proceso es el uso de la estad칤stica para analizar textos y extraer informaci칩n valiosa. En este art칤culo, profundizaremos en c칩mo las herramientas estad칤sticas se aplican en NLP para enriquecer nuestra comprensi칩n del lenguaje.

쯈u칠 herramientas y librer칤as son esenciales para el an치lisis estad칤stico en texto?
Para un an치lisis estad칤stico efectivo en NLP usando Python, se necesitan varias librer칤as y herramientas cruciales. Aqu칤 te presento algunas de las m치s destacadas:

NLTK (Natural Language Toolkit): Es una de las librer칤as m치s utilizadas para trabajar con lenguaje natural en Python. Proporciona herramientas para tokenizaci칩n, etiquetado, an치lisis de texto y m치s.

Matplotlib: Esta es una librer칤a de visualizaci칩n de datos en Python que es muy 칰til para presentar gr치ficos y distribuciones.

NumPy: Una librer칤a fundamental cuando se trabaja con 치lgebra lineal y matem치ticas avanzadas en Python. Es esencial para el manejo eficiente de matrices y vectores. Existen cursos dedicados a profundizar en su uso.

Adem치s, es frecuente trabajar con datasets preexistentes, como el books dataset de NLTK, que contiene libros tokenizados en ingl칠s, siendo uno de los m치s 칰tiles para procesamiento de texto.

쮺칩mo comenzar a tokenizar y calcular m칠tricas del texto?
Cuando se trabaja con textos, un paso inicial cr칤tico es la tokenizaci칩n, es decir, dividir el texto en palabras o tokens individuales. Una vez tokenizado, se pueden calcular m칠tricas 칰tiles que nos proporcionan informaci칩n sobre el texto, como la longitud del texto y la riqueza l칠xica.

Tokenizaci칩n: Con NLTK, puedes f치cilmente importar datasets y comenzar a tokenizar. Por ejemplo, usando from nltk import book y luego consultando los tokens espec칤ficos para un texto.
from nltk.book import *
tokens = text1[:10]  # Obt칠n los primeros 10 tokens
Longitud y riqueza l칠xica: La longitud del texto se refiere al n칰mero total de tokens, mientras que la riqueza l칠xica se define como el n칰mero de palabras 칰nicas dividido por el total de palabras.
longitud_texto = len(text1)
vocabulario = set(text1)
riqueza_lexica = len(vocabulario) / longitud_texto
쮺칩mo definir funciones 칰tiles para el an치lisis de texto?
Definir funciones en Python facilita la reutilizaci칩n del c칩digo y permite calcular m칠tricas como la riqueza l칠xica y el porcentaje de uso de palabras. Veamos c칩mo definir algunas funciones 칰tiles:

Funci칩n para calcular riqueza l칠xica:
def riqueza_lexica(texto):
    vocabulario = set(texto)
    return len(vocabulario) / len(texto)
Esta funci칩n nos ayuda a calcular qu칠 tan diverso es el vocabulario del texto.

Funci칩n para calcular el porcentaje de una palabra:
def porcentaje_palabra(palabra, texto):
    return 100 * texto.count(palabra) / len(texto)
Utilizando esta funci칩n, podemos determinar qu칠 tan com칰n es una palabra en particular dentro de un texto, expresado como un porcentaje.

En conclusi칩n, el uso de herramientas estad칤sticas en NLP es esencial para el an치lisis y comprensi칩n profunda de textos. Estas t칠cnicas nos permiten desentra침ar patrones del lenguaje y realizar interpretaciones que de otra manera no ser칤an posibles. Con pr치ctica y curiosidad, podr치s sacar el m치ximo provecho a estas herramientas y funciones para tus futuros proyectos en procesamiento del lenguaje natural.

















Siguiente recomendado: 

Curso de introducci칩n a Machine learning. 
Curso de expresiones regulares

